{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "# Computes mae in batches so that we don't have memory issue in big dataset\n",
    "def compute_MAE(sess, train_sparse, output_layer, bsize):\n",
    "\n",
    "    loss_mae = 0\n",
    "    tot_users = train_sparse.shape[0]\n",
    "    train_sparse = train_sparse.tocsr()\n",
    "    # print(tot_users)\n",
    "\n",
    "    for i in range(int(tot_users/bsize)+1):\n",
    "\n",
    "        to = (i+1)*bsize\n",
    "        if to > tot_users:\n",
    "            to = tot_users\n",
    "            \n",
    "        epoch_x = train_sparse[ i*bsize : to ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "\n",
    "        output_train = sess.run(output_layer, feed_dict={X:epoch_x})\n",
    "        loss_mae += np.sum(abs(output_train - epoch_x))\n",
    "\n",
    "    mae = loss_mae / (train_sparse.shape[0]*train_sparse.shape[1])\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_RMSE(sess, train_sparse, output_layer, bsize):\n",
    "\n",
    "    loss_rmse = 0\n",
    "    tot_users = train_sparse.shape[0]\n",
    "    train_sparse = train_sparse.tocsr()\n",
    "    # print(tot_users)\n",
    "\n",
    "    for i in range(int(tot_users/bsize)+1):\n",
    "\n",
    "        to = (i+1)*bsize\n",
    "        if to > tot_users:\n",
    "            to = tot_users\n",
    "            \n",
    "        epoch_x = train_sparse[ i*bsize : to ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "\n",
    "        output_train = sess.run(output_layer, feed_dict={X:epoch_x})\n",
    "        loss_rmse += np.sum(np.square(output_train - epoch_x))\n",
    "\n",
    "    rmse = loss_rmse / (train_sparse.shape[0]*train_sparse.shape[1])\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('../datasets/ml-latest-small/ratings.csv', sep=\",\")\n",
    "ratings_df = ratings_df.drop('timestamp', axis=1)\n",
    "# ratings_df = ratings_df.head(len(ratings_df)//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "r = ratings_df['rating'].values.astype(float)\n",
    "min_max_scaler = sk.preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "ratings_df['rating'] = df_normalized\n",
    "\n",
    "# Preprocessing\n",
    "np_users = ratings_df.userId.values\n",
    "np_items = ratings_df.movieId.values\n",
    "\n",
    "unique_users = np.unique(np_users)\n",
    "unique_items = np.unique(np_items)\n",
    "\n",
    "n_users = unique_users.shape[0]\n",
    "n_items = unique_items.shape[0]\n",
    "\n",
    "max_item = unique_items[-1]\n",
    "# Reconstruct the ratings set's user/movie indices\n",
    "np_users = ratings_df.userId.values\n",
    "np_users[:] -= 1 # Make users zero-indexed\n",
    "# print(np_users)\n",
    "\n",
    "# Mapping unique items down to an array 0..n_items-1\n",
    "z = np.zeros(max_item+1, dtype=int)\n",
    "z[unique_items] = np.arange(n_items)\n",
    "movies_map = z[np_items]\n",
    "\n",
    "np_ratings = ratings_df.rating.values\n",
    "# print(np_ratings.shape[0])\n",
    "ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "ratings[:, 0] = np_users\n",
    "ratings[:, 1] = movies_map\n",
    "ratings[:, 2] = np_ratings\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(ratings, train_size=0.8)\n",
    "# print(X_train)\n",
    "\n",
    "# Ignoring timestamp\n",
    "user_train, movie_train, rating_train = zip(*X_train)\n",
    "train_sparse = coo_matrix((rating_train, (user_train, movie_train)), shape=(n_users, n_items))\n",
    "# print(train_sparse.shape)\n",
    "\n",
    "user_test, movie_test, rating_test = zip(*X_test)\n",
    "test_sparse = coo_matrix((rating_test, (user_test, movie_test)), shape=(n_users, n_items))\n",
    "# print(test_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "num_input = n_items\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}\n",
    "# Encoder Hidden layer with sigmoid activation #1\n",
    "en_layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))\n",
    "# Encoder Hidden layer with sigmoid activation #2\n",
    "encoder_op = tf.nn.sigmoid(tf.add(tf.matmul(en_layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "\n",
    "# Decoder Hidden layer with sigmoid activation #1\n",
    "c = tf.nn.sigmoid(tf.add(tf.matmul(encoder_op, weights['decoder_h1']), biases['decoder_b1']))\n",
    "# Decoder Hidden layer with sigmoid activation #2\n",
    "decoder_op  = tf.nn.sigmoid(tf.add(tf.matmul(en_layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "# Define evaluation metrics\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "hm_epochs = 10\n",
    "tot_users = train_sparse.shape[0]\n",
    "\n",
    "train_sparse = train_sparse.tocsr()\n",
    "test_sparse = test_sparse.tocsr()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "sess.run(local_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 0.5005063062963306 MAE test 0.5022091392010846\n",
      "RMSE train 0.6009669974577159 RMSE test 0.602466431328224\n",
      "MAE train 0.500504807364888 MAE test 0.502207601914092\n",
      "RMSE train 0.6009636263599696 RMSE test 0.6024634662666482\n",
      "MAE train 0.5005029569693995 MAE test 0.5022057041529595\n",
      "RMSE train 0.6009594644084683 RMSE test 0.6024598056858346\n",
      "MAE train 0.5005006728220612 MAE test 0.5022033614140128\n",
      "RMSE train 0.600954326041148 RMSE test 0.602455286433861\n",
      "MAE train 0.5004978532391244 MAE test 0.5022004694121223\n",
      "RMSE train 0.6009479821416054 RMSE test 0.6024497070859958\n",
      "MAE train 0.5004978532391244 MAE test 0.5022004694121223\n",
      "RMSE train 0.6009479821416054 RMSE test 0.6024497070859958\n",
      "Epoch 9 / 10 loss: 0.7219357490539551\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(int(tot_users/batch_size)):\n",
    "        epoch_x = train_sparse[ i*batch_size : (i+1)*batch_size ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "        _, c = sess.run([optimizer, loss],feed_dict={X: epoch_x})\n",
    "        epoch_loss += c\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print('MAE train', compute_MAE(sess, train_sparse, y_pred, batch_size), \n",
    "            'MAE test', compute_MAE(sess, test_sparse, y_pred, batch_size))\n",
    "\n",
    "        print('RMSE train', np.sqrt(compute_RMSE(sess, train_sparse, y_pred, batch_size)), \n",
    "            'RMSE test', np.sqrt(compute_RMSE(sess, test_sparse, y_pred, batch_size)))\n",
    "\n",
    "#         save_weights(sess, hidden_1_layer_vals, output_layer_vals)\n",
    "print('MAE train', compute_MAE(sess, train_sparse, y_pred, batch_size), \n",
    "    'MAE test', compute_MAE(sess, test_sparse, y_pred, batch_size))\n",
    "\n",
    "print('RMSE train', np.sqrt(compute_RMSE(sess, train_sparse, y_pred, batch_size)), \n",
    "    'RMSE test', np.sqrt(compute_RMSE(sess, test_sparse, y_pred, batch_size)))\n",
    "\n",
    "print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)\n",
    "#     save_weights(sess, hidden_1_layer_vals, output_layer_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
