{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Computes mae in batches so that we don't have memory issue in big dataset\n",
    "def compute_MAE(sess, train_sparse, output_layer, bsize):\n",
    "\n",
    "    loss_mae = 0\n",
    "    tot_users = train_sparse.shape[0]\n",
    "    train_sparse = train_sparse.tocsr()\n",
    "    # print(tot_users)\n",
    "\n",
    "    for i in range(int(tot_users/bsize)+1):\n",
    "\n",
    "        to = (i+1)*bsize\n",
    "        if to > tot_users:\n",
    "            to = tot_users\n",
    "            \n",
    "        epoch_x = train_sparse[ i*bsize : to ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "\n",
    "        output_train = sess.run(output_layer, feed_dict={input_layer:epoch_x})\n",
    "        loss_mae += np.sum(abs(output_train - epoch_x))\n",
    "\n",
    "    mae = loss_mae / (train_sparse.shape[0]*train_sparse.shape[1])\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_RMSE(sess, train_sparse, output_layer, bsize):\n",
    "\n",
    "    loss_rmse = 0\n",
    "    tot_users = train_sparse.shape[0]\n",
    "    train_sparse = train_sparse.tocsr()\n",
    "    # print(tot_users)\n",
    "\n",
    "    for i in range(int(tot_users/bsize)+1):\n",
    "\n",
    "        to = (i+1)*bsize\n",
    "        if to > tot_users:\n",
    "            to = tot_users\n",
    "            \n",
    "        epoch_x = train_sparse[ i*bsize : to ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "\n",
    "        output_train = sess.run(output_layer, feed_dict={input_layer:epoch_x})\n",
    "        loss_rmse += np.sum(np.square(output_train - epoch_x))\n",
    "\n",
    "    rmse = loss_rmse / (train_sparse.shape[0]*train_sparse.shape[1])\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def save_weights(sess, hidden_vals, output_vals):\n",
    "\n",
    "    v1 = sess.run(hidden_vals)\n",
    "    f = open(\"weights-hidden.pkl\",\"wb\")\n",
    "    pickle.dump(v1,f)\n",
    "    f.close()\n",
    "\n",
    "    v2 = sess.run(output_vals)\n",
    "    f = open(\"weights-output.pkl\",\"wb\")\n",
    "    pickle.dump(v2,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ratings data\n",
    "ratings_df = pd.read_csv('../datasets/ml-latest/ratings.csv', sep=\",\")\n",
    "\n",
    "# Making the dataset a little bit smaller due to lack of memory resources\n",
    "# ratings_df = ratings_df.head(len(ratings_df)/10)\n",
    "# print(ratings_df)\n",
    "\n",
    "# Preprocessing\n",
    "np_users = ratings_df.userId.values\n",
    "np_items = ratings_df.movieId.values\n",
    "\n",
    "unique_users = np.unique(np_users)\n",
    "unique_items = np.unique(np_items)\n",
    "\n",
    "n_users = unique_users.shape[0]\n",
    "n_items = unique_items.shape[0]\n",
    "\n",
    "# print(n_users)\n",
    "# print(n_items)\n",
    "\n",
    "max_item = unique_items[-1]\n",
    "\n",
    "# Reconstruct the ratings set's user/movie indices\n",
    "np_users = ratings_df.userId.values\n",
    "np_users[:] -= 1 # Make users zero-indexed\n",
    "# print(np_users)\n",
    "\n",
    "# Mapping unique items down to an array 0..n_items-1\n",
    "z = np.zeros(max_item+1, dtype=int)\n",
    "z[unique_items] = np.arange(n_items)\n",
    "movies_map = z[np_items]\n",
    "\n",
    "np_ratings = ratings_df.rating.values\n",
    "# print(np_ratings.shape[0])\n",
    "ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "ratings[:, 0] = np_users\n",
    "ratings[:, 1] = movies_map\n",
    "ratings[:, 2] = np_ratings\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(ratings, train_size=0.8)\n",
    "# print(X_train)\n",
    "\n",
    "# Ignoring timestamp\n",
    "user_train, movie_train, rating_train = zip(*X_train)\n",
    "train_sparse = coo_matrix((rating_train, (user_train, movie_train)), shape=(n_users, n_items))\n",
    "# print(train_sparse.shape)\n",
    "\n",
    "user_test, movie_test, rating_test = zip(*X_test)\n",
    "test_sparse = coo_matrix((rating_test, (user_test, movie_test)), shape=(n_users, n_items))\n",
    "# print(test_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Graph - Autoencoder with one hidden layer\n",
    "\n",
    "![autoencoder with one layer](../Images/autoencoders-1layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding how many nodes each layer should have - Depending on the dataset's size\n",
    "movies_size = 53889     #9724  \n",
    "n_nodes_inpl = movies_size\n",
    "n_nodes_hl1  = 256\n",
    "n_nodes_outl = movies_size\n",
    "\n",
    "# with tf.device('/cpu:0'):\n",
    "\n",
    "# Initialize them randomly or use pre-computed ones\n",
    "if os.path.isfile('weights-hidden.pkl'):\n",
    "    weights_hidden = np.load(\"weights-hidden.pkl\",\"wb\", allow_pickle=True)\n",
    "    # print(weights_hidden['weights'])\n",
    "    hidden_1_layer_vals = {'weights':tf.Variable(weights_hidden['weights'])}\n",
    "else:\n",
    "    hidden_1_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_inpl+1, n_nodes_hl1]))}    \n",
    "\n",
    "if os.path.isfile('weights-output.pkl'):\n",
    "    weights_output = np.load(\"weights-output.pkl\",\"wb\", allow_pickle=True)\n",
    "    # print(weights_output['weights'])\n",
    "    output_layer_vals = {'weights':tf.Variable(weights_output['weights'])}\n",
    "else:\n",
    "    output_layer_vals = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1+1, n_nodes_outl]))}\n",
    "\n",
    "\n",
    "\n",
    "input_layer = tf.placeholder('float', [None, movies_size])\n",
    "\n",
    "input_layer_const = tf.fill([tf.shape(input_layer)[0], 1], 1.0)\n",
    "input_layer_concat = tf.concat([input_layer, input_layer_const], 1)\n",
    "\n",
    "layer_1 = tf.nn.sigmoid(tf.matmul(input_layer_concat, hidden_1_layer_vals['weights']))\n",
    "\n",
    "layer1_const = tf.fill( [tf.shape(layer_1)[0], 1], 1.0)\n",
    "layer_concat =  tf.concat([layer_1, layer1_const], 1)\n",
    "\n",
    "output_layer = tf.matmul(layer_concat, output_layer_vals['weights'])\n",
    "\n",
    "output_true = tf.placeholder('float', [None, movies_size])\n",
    "meansq = tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "\n",
    "learn_rate = 0.1   # learning rate\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)\n",
    "\n",
    "# initializing variables and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "writer.close()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse = train_sparse.tocsr()\n",
    "test_sparse = test_sparse.tocsr()\n",
    "\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(int(tot_users/batch_size)):\n",
    "        epoch_x = train_sparse[ i*batch_size : (i+1)*batch_size ]\n",
    "        epoch_x = epoch_x.toarray()\n",
    "        _, c = sess.run([optimizer, meansq],feed_dict={input_layer: epoch_x, output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "  \n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print('MAE train', compute_MAE(sess, train_sparse, output_layer, batch_size), \n",
    "            'MAE test', compute_MAE(sess, test_sparse, output_layer, batch_size))\n",
    "        \n",
    "        print('RMSE train', np.sqrt(compute_RMSE(sess, train_sparse, output_layer, batch_size)), \n",
    "            'RMSE test', np.sqrt(compute_RMSE(sess, test_sparse, output_layer, batch_size)))\n",
    "\n",
    "        save_weights(sess, hidden_1_layer_vals, output_layer_vals)\n",
    "\n",
    "print('MAE train', compute_MAE(sess, train_sparse, output_layer, batch_size), \n",
    "    'MAE test', compute_MAE(sess, test_sparse, output_layer, batch_size))\n",
    "\n",
    "print('RMSE train', np.sqrt(compute_RMSE(sess, train_sparse, output_layer, batch_size)), \n",
    "    'RMSE test', np.sqrt(compute_RMSE(sess, test_sparse, output_layer, batch_size)))\n",
    "\n",
    "print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)\n",
    "save_weights(sess, hidden_1_layer_vals, output_layer_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a user\n",
    "sample_user = X_test.iloc[99,:]\n",
    "# get the predicted ratings\n",
    "sample_user_pred = sess.run(output_layer, feed_dict={input_layer:[sample_user]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
