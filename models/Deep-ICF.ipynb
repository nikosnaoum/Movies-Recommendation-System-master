{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os as os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from time import time\n",
    "\n",
    "# Batch handling\n",
    "def remove_item(feature_mask, users, item):\n",
    "    flag = 0\n",
    "    for i in range(len(users)):\n",
    "        if users[i] == item:\n",
    "            users[i] = users[-1]\n",
    "            users[-1] = feature_mask\n",
    "            flag = 1\n",
    "            break\n",
    "    return len(users) - flag\n",
    "\n",
    "def add_mask(feature_mask, features, num_max):\n",
    "    #uniformalize the length of each batch\n",
    "    for i in range(len(features)):\n",
    "        features[i] = features[i] + [feature_mask] * (num_max+1 - len(features[i]))\n",
    "    return features\n",
    "\n",
    "\n",
    "def shuffle(trainMatrix, trainList, _num_negatives):\n",
    "\n",
    "    # Get train data user\n",
    "    _user_input, _item_input, _labels, _batch_length = [],[],[],[]\n",
    "    train = trainMatrix\n",
    "    trainList = trainList\n",
    "    \n",
    "    _num_items = train.shape[1]\n",
    "    _num_users = train.shape[0]\n",
    "    \n",
    "    print(_num_users)\n",
    "    print(_num_items)\n",
    "    \n",
    "    for u in range(_num_users):\n",
    "        if u == 0:\n",
    "            _batch_length.append((1+_num_negatives) * len(trainList[u]))\n",
    "        else:\n",
    "            _batch_length.append((1+_num_negatives) * len(trainList[u])+_batch_length[u-1])\n",
    "        for i in trainList[u]:\n",
    "            # positive instance\n",
    "            _user_input.append(u)\n",
    "            _item_input.append(i)\n",
    "            _labels.append(1)\n",
    "            # negative instances\n",
    "            for t in range(_num_negatives):\n",
    "                j = np.random.randint(_num_items)\n",
    "                while j in trainList[u]:\n",
    "                    j = np.random.randint(_num_items)\n",
    "                _user_input.append(u)\n",
    "                _item_input.append(j)\n",
    "                _labels.append(0)\n",
    "\n",
    "    _num_batch = len(_batch_length)\n",
    "     \n",
    "    # Preprocess\n",
    "    user_input_list, num_idx_list, item_input_list, labels_list = [], [], [], []\n",
    "    for i in range(int(_num_batch)):\n",
    "        \n",
    "        # Train Batch User\n",
    "        user_list1, num_list1, item_list1, labels_list1 = [],[],[],[]\n",
    "        if i == 0:\n",
    "            begin = 0\n",
    "        else:\n",
    "            begin = _batch_length[i-1]\n",
    "        batch_index = list(range(begin, _batch_length[i]))\n",
    "        np.random.shuffle(batch_index)\n",
    "        \n",
    "        for idx in batch_index:\n",
    "            user_idx = _user_input[idx]\n",
    "            item_idx = _item_input[idx]\n",
    "            nonzero_row = []\n",
    "            nonzero_row += trainList[user_idx]\n",
    "            num_list1.append(remove_item(_num_items, nonzero_row, item_idx))\n",
    "            user_list1.append(nonzero_row)\n",
    "            item_list1.append(item_idx)\n",
    "            labels_list1.append(_labels[idx])\n",
    "        user_input = np.array(add_mask(_num_items, user_list1, max(num_list1)))\n",
    "        num_idx = np.array(num_list1)\n",
    "        item_input = np.array(item_list1)\n",
    "        labels = np.array(labels_list1)\n",
    "        \n",
    "        ui, ni, ii, l = user_input, num_idx, item_input, labels\n",
    "     \n",
    "    \n",
    "        user_input_list.append(ui)\n",
    "        num_idx_list.append(ni)\n",
    "        item_input_list.append(ii)\n",
    "        labels_list.append(l)\n",
    "\n",
    "\n",
    "    return  user_input_list, num_idx_list, item_input_list, labels_list\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Dataset\n",
    "\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "\n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "\n",
    "def load_training_file_as_matrix(filename):\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0):\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()\n",
    "    print(\"already load the trainMatrix...\")\n",
    "    return mat\n",
    "\n",
    "\n",
    "def load_training_file_as_list(filename):\n",
    "    # Get number of users and items\n",
    "    u_ = 0\n",
    "    lists, items = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        index = 0\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            if u_ < u:\n",
    "                index = 0\n",
    "                lists.append(items)\n",
    "                items = []\n",
    "                u_ += 1\n",
    "            index += 1\n",
    "            if index < 300:\n",
    "                items.append(i)\n",
    "            line = f.readline()\n",
    "    lists.append(items)\n",
    "    print(\"already load the trainList...\")\n",
    "    return lists\n",
    "\n",
    "\n",
    "# def batch_norm_layer(x, train_phase, scope_bn):\n",
    "#     bn_train = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n",
    "#                           is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "#     bn_inference = batch_norm(x, decay=0.9, center=True, scale=True, updates_collections=None,\n",
    "#                               is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "#     z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "#     return z\n",
    "\n",
    "\n",
    "def training_batch(batch_index, model, sess, batches):\n",
    "    for index in batch_index:\n",
    "        user_input, num_idx, item_input, labels = data.batch_gen(batches, index)\n",
    "        feed_dict = {model.user_input: user_input, model.num_idx: num_idx[:, None],\n",
    "                     model.item_input: item_input[:, None],\n",
    "                     model.labels: labels[:, None], model.is_train_phase: True}\n",
    "        sess.run(model.optimizer, feed_dict)\n",
    "\n",
    "\n",
    "def training_loss(model, sess, batches):\n",
    "    train_loss = 0.0\n",
    "    num_batch = len(batches[1])\n",
    "    for index in range(num_batch):\n",
    "        user_input, num_idx, item_input, labels = data.batch_gen(batches, index)\n",
    "        feed_dict = {model.user_input: user_input, model.num_idx: num_idx[:, None],\n",
    "                     model.item_input: item_input[:, None],\n",
    "                     model.labels: labels[:, None], model.is_train_phase: True}\n",
    "        train_loss += sess.run(model.loss, feed_dict)\n",
    "    return train_loss / num_batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already load the trainMatrix...\n",
      "already load the trainList...\n"
     ]
    }
   ],
   "source": [
    "path = \"../datasets/Data/ml-1m\"\n",
    "\n",
    "trainMatrix = load_training_file_as_matrix(path + \".train.rating\")\n",
    "trainList = load_training_file_as_list(path + \".train.rating\")\n",
    "testRatings = load_rating_file_as_list(path + \".test.rating\")\n",
    "testNegatives = load_negative_file(path + \".test.negative\")\n",
    "assert len(testRatings) == len(testNegatives)\n",
    "num_users, num_items = trainMatrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "embedding_size = 16\n",
    "alpha = 0.5\n",
    "verbose = 1\n",
    "n_hidden = [64, 32, 16]\n",
    "regs = [1e-06, 1e-06]\n",
    "train_loss = 1\n",
    "reg_W = [0.1,0.1,0.1,0.1]\n",
    "batch_choice = \"user\"\n",
    "use_batch_norm = False\n",
    "num_negatives = 2\n",
    "lambda_bilinear = 1 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    "gamma_bilinear = 1 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Placeholders\n",
    "user_input = tf.placeholder(tf.int32, shape=[None, None])  # the index of users\n",
    "num_idx = tf.placeholder(tf.float32, shape=[None, 1])  # the number of items rated by users\n",
    "item_input = tf.placeholder(tf.int32, shape=[None, 1])  # the index of items\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 1])  # the ground truth\n",
    "is_train_phase = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "# Creating Variables\n",
    "c1 = tf.Variable(tf.truncated_normal(shape=[num_items, embedding_size], \n",
    "                                     mean=0.0, stddev=0.01), name='c1', dtype=tf.float32)\n",
    "c2 = tf.constant(0.0, tf.float32, [1, embedding_size], name='c2')\n",
    "\n",
    "embedding_Q_ = tf.concat([c1, c2], 0, name='embedding_Q_')\n",
    "embedding_Q = tf.Variable(tf.truncated_normal(shape=[num_items, embedding_size], \n",
    "                                              mean=0.0, stddev=0.01), name='embedding_Q', dtype=tf.float32)\n",
    "\n",
    "bias = tf.Variable(tf.zeros(num_items), name='bias')\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden[-1], 1], mean=0, \n",
    "                                stddev=np.sqrt(2.0 / (n_hidden[-1] + 1))), name='weights_out')}\n",
    "\n",
    "biases = {'out': tf.Variable(tf.random_normal([1]), name='biases_out')}\n",
    "n_hidden_0 = embedding_size\n",
    "\n",
    "for i in range(len(n_hidden)):\n",
    "    if i > 0:\n",
    "        n_hidden_0 = n_hidden[i - 1]\n",
    "    n_hidden_1 = n_hidden[i]\n",
    "    weights['h%d' % i] = tf.Variable(tf.random_normal([n_hidden_0, n_hidden_1], mean=0, \n",
    "                                    stddev=np.sqrt(2.0 / (n_hidden_0 + n_hidden_1))), name='weights_h%d' % i)\n",
    "    biases['b%d' % i] = tf.Variable(tf.random_normal([n_hidden_1]), name='biases_b%d' % i)\n",
    "\n",
    "    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "# Creating Inference    \n",
    "embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(embedding_Q_, user_input), 1)\n",
    "embedding_q = tf.reduce_sum(tf.nn.embedding_lookup(embedding_Q, item_input), 1)\n",
    "bias_i = tf.nn.embedding_lookup(bias, item_input)\n",
    "coeff = tf.pow(num_idx, -tf.constant(alpha, tf.float32, [1]))\n",
    "embedding_p = coeff * embedding_p\n",
    "\n",
    "layer1 = tf.multiply(embedding_p, embedding_q)\n",
    "for i in range(len(n_hidden)):\n",
    "    layer1 = tf.add(tf.matmul(layer1, weights['h%d' % i]), biases['b%d' % i])\n",
    "#     if use_batch_norm:\n",
    "#         layer1 = batch_norm_layer(layer1, train_phase=is_train_phase, scope_bn='bn_%d' % i)\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "out_layer = tf.matmul(layer1, weights['out']) + biases['out']\n",
    "\n",
    "output = tf.sigmoid(tf.add_n([out_layer, bias_i]))\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "# Creating loss\n",
    "loss = tf.losses.log_loss(labels, output) + \\\n",
    "            lambda_bilinear * tf.reduce_sum(tf.square(embedding_Q)) + \\\n",
    "            gamma_bilinear * tf.reduce_sum(tf.square(embedding_Q_))\n",
    "\n",
    "for i in range(min(len(n_hidden), len(reg_W))):\n",
    "    if reg_W[i] > 0:\n",
    "        loss = loss + reg_W[i] * tf.reduce_sum(tf.square(weights['h%d' % i]))\n",
    "        \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "# Creating optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate,\n",
    "                initial_accumulator_value=1e-8).minimize(loss)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n",
      "3706\n"
     ]
    }
   ],
   "source": [
    "# weight_path = '../datasets/Pretraining/ml-1m/16/alpha0.0.ckpt'\n",
    "# saver = tf.train.Saver([c1, embedding_Q, bias])\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# # Use pretrained\n",
    "# saver.restore(sess, weight_path)\n",
    "# p_c1, p_e_Q, p_b = sess.run([c1, embedding_Q, bias])\n",
    "\n",
    "# c1 = tf.Variable(p_c1, dtype=tf.float32, trainable=True, name='c1')\n",
    "# embedding_Q_ = tf.concat([model.c1, model.c2], 0, name='embedding_Q_')\n",
    "# embedding_Q = tf.Variable(p_e_Q, dtype=tf.float32, trainable=True, name='embedding_Q')\n",
    "# bias = tf.Variable(p_b, dtype=tf.float32, trainable=True, name='embedding_Q')\n",
    "\n",
    "# logging.info(\"using pretrained variables\")\n",
    "# print(\"using pretrained variables\")\n",
    "\n",
    "\n",
    "\n",
    "# initialize for training batches\n",
    "batch_begin = time()\n",
    "batches = shuffle(trainMatrix, trainList, num_negatives)\n",
    "batch_time = time() - batch_begin\n",
    "\n",
    "print(batches)\n",
    "# num_batch = len(batches[1])\n",
    "# batch_index = list(range(num_batch))\n",
    "\n",
    "\n",
    "\n",
    "# # # initialize the evaluation feed_dicts\n",
    "# # testDict = evaluate.init_evaluate_model(model, sess, dataset.testRatings, dataset.testNegatives,\n",
    "# #                                         dataset.trainList)\n",
    "\n",
    "\n",
    "\n",
    "# best_hr, best_ndcg = 0, 0\n",
    "# # train by epoch\n",
    "# for epoch_count in range(epochs):\n",
    "\n",
    "#     train_begin = time()\n",
    "#     for index in batch_index:\n",
    "#         user_input, num_idx, item_input, labels = [(batches[r])[index] for r in range(4)] \n",
    "        \n",
    "#         # Train a  batch\n",
    "#         feed_dict = {user_input: user_input, num_idx: num_idx[:, None],\n",
    "#                      item_input: item_input[:, None],\n",
    "#                      labels: labels[:, None], is_train_phase: True}\n",
    "#         sess.run(optimizer, feed_dict)\n",
    "#     train_time = time() - train_begin\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     #################################################################################\n",
    "# #     if epoch_count % verbose == 0:\n",
    "# #         train_loss = 0.0\n",
    "# #         num_batch = len(batches[1])\n",
    "        \n",
    "# #         for index in range(num_batch):\n",
    "# #             user_input, num_idx, item_input, labels = [(batches[r])[index] for r in range(4)]\n",
    "            \n",
    "# #             feed_dict = {user_input: user_input, model.num_idx: num_idx[:, None],\n",
    "# #                          item_input: item_input[:, None],\n",
    "# #                          labels: labels[:, None], model.is_train_phase: True}\n",
    "# #             train_loss += sess.run(loss, feed_dict)\n",
    "\n",
    "# #         (hits, ndcgs, losses) = evaluate.eval(model, sess, dataset.testRatings, dataset.testNegatives, testDict)\n",
    "# #         hr, ndcg, test_loss = np.array(hits).mean(), np.array(ndcgs).mean(), np.array(losses).mean()\n",
    "#     #################################################################################\n",
    "\n",
    "#     batch_begin = time()\n",
    "#     batches = shuffle(trainMatrix, trainList, batch_size, num_negatives)\n",
    "#     np.random.shuffle(batch_index)\n",
    "#     batch_time = time() - batch_begin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
